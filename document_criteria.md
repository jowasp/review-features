#Originality: 

In order to measure the originality of the document, we can automate this process using a Plagiarism checker. Documents that copy paste other author’s information and publish it as their own infringe in plagiarism/copyright issues and this should not be accepted, only if the original author authorizes to publish the information under the OWASP documentation or references have been properly addressed.

#Grammar:

This can also be done with tools such as “PlagTracker”. We can determine a minimum amount of grammar mistakes (the tool can measure this in percentages). We could determine that the document should not have more than 15% of grammatical mistakes for example

#Accessibility: 

How easy and an accessible is the document? Also with Accessibility we mean which projects have a “mailing list” responding to users or feedback form for readers to provide their opinion regarding the document and answer to their questions. An extra point for accessibility will be considered if Project leaders have placed their content in containers such as repositories or wikis to facilitated collaboration among authors and update their information.

#Open Source: 
Since everything OWASP puts out there is free and open source, it is important to validate that all of our documentation projects live up to this standard. This means not only slapping an open source license on the document, but also verifying that all of the materials that were used to produce the document were properly licensed as well (nothing proprietary).


#Publicly Available: 

It's counter-intuitive that we would produce any project that would not be made publicly available, but it's happened before. In order for a project to be Flagship, it needs to be readily downloadable by anyone who wants it. No guestbook or other forms of data collection for downloads either. This is implicit in "Accessibility" criteria, but needs to be specifically called out.

#Credibility: 

This is about trustworthiness and expertise. That means evaluating the authors sources deeply and for this we also need experts evaluating the subject treated in the document

#Accuracy: 

Are there errors in the subjects/code treated? How correct is the information? In order to measure this part we will execute a research to find discussions of readers about the document.

#Relevance to the audience: 

we will use the amount of hits/downloads/searches of wiki page /or document  has received since its inconception. This is just a key indicator of the level of interest to the audience. Measuring amount of
downloads is also an indicator of interest.

#Relevance to the subject (security)

this can be measured using the results of the content audit. How many documents are handling the same subject and which ones are unique to the matter.

#Defined and Repeatable: 

labeling something as Flagship is a promise that it will be supportable long-
term. Project leaders and contributors will come and go, but we need to make sure that the project can live on. To this extent, we need to make sure that not only the document, but also the reference materials that went into creating the document, and the process to assemble those materials are documented so that it can be repeated with future iterations.

#Formatting/Branding:

If Flagship project status truly represents the best we have, then we should put our best face forward not only in content, but also in format. This means that we need to develop a standardized look and feel for all of our documentation projects.
